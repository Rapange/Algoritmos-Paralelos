4.1
a) 1 536 / 128 = 12 bloques
b) 1 536 / 256 = 6 bloques
c) 1 536 / 512 = 3 bloques
d) 1536 / 1025 = 1 bloque
El mejor sería 512 hilos por bloque, la c.
4.2
Ceil (2 000 / 512) = 4
Entonces, 512 * 4 = 2048 hilos en la grilla.
4.3
Los primeros 2000 hilos harán lo mismo y los siguientes 48 harán todos lo mismo. Pero, 2000/32 = 62 warps, y quedan 16 hilos libres. Estos 16 hilos libres más los 16 hilos después del 2000 estarán en un warp y harán cosas diferentes.
Por ende, 1 warp tendrá divergencia.
4.4
Para 3.0, se tiene que la cantidad de hilos por bloque es 1024. Por ende, las dimensiones del bloque serían 32 x 32 = 1024, y las de la rejilla 13 x 29
4.5
Por el alto se tiene 32 x 13 = 416. Sobran 16 filas de hilos por bloque. En total sobran 29 x 16 x 32 = 14 848 hilos.
Por el ancho se tiene 32 x 29 = 928. Sobran 28 columnas de hilos por bloque. En total sobran 13 x 28 x 32 = 11 648 hilos.
En total sobran: 26 496 hilos.
4.6
El tiempo total de ejecución de todos los hilos es: 2.0 + 2.3 + 3.0 + 2.8 + 2.4 + 1.9 + 2.6 + 2.9 = 17 ms. Cada hilo espera hasta el segundo 3.0, se tiene: 1.0 + 0.7 + 0 + 0.2 + 0.6 + 1.1 + 0.4 + 0.1 = 4.1 ms. Esto es: (4.1 x 100) / 17 = 24.118%
4.7 c y f posibles, d y e no por bloques e hilos, a y b no por hilos. 
one SM can have 

(1) 8 active thread-block at most AND

(2) 1024 active threads at most 
4.8
Le diría que es una mala idea ya no se puede asumir que el warp siempre será de tamaño 32.
4.9 No está aprovechando bien sus recursos, tiene más hilos de lo permitido.
4.10 
a) Cuando BLOCK_SIZE esté entre 1 y 5 ya que no hay sincronización y todo el bloque debe estar en un warp para que funcione bien.
b) Se agrega un __syncthreads() entre el escribir y leer el arreglo A.
5.1 No se puede, ya que cada elemento de ambas matrices es accedido una sola vez.
5.3 Si se olvida el primero, Pvalue puede estar sumando valores basura. Si se olvida el segundo, Pvalue puede llegar a tener un resultado erróneo.
5.4 Si se usa la memoria compartida, solo se tendrá que leer una vez de la memoria global. Sin embargo, si se usarían los registros, se harían varias lecturas para una misma operación.
5.5 1/32
5.6 512 000
5.7 1000
5.8 L1 Cache es para CPU, mientras que shared memory es para GPU.
5.9
a) N veces
b) N / T veces
5.10
a) 
36 / (200 * 10^9) = 0.18e^(-9)s
 (7*4) / (100 * 10^9) = 0.28e^(-9)s
Como 0.28e^(-9)s > 0.18e^(-9)s, entonces es orientado a memoria.
b)
36 / (300 * 10^9) = 0.12e^(-9)s
 (7*4) / (250 * 10^9) = 0.112e^(-9)s
Como 0.12e^(-9)s > 0.112e^(-9)s, entonces es orientado a cálculos.
5.11 16 x 32-bit for compute capability 1.x

6.1 Sí, hay más operaciones aritméticas que se hacen. El recurso de la memoria local.
6.2 El primero.
6.6 N porque los hilos van a acceder por filas.
6.7 Ambas matrices porque van a acceder por filas.
6.8 32 warps (todos)
6.9 0 warps.
6.11
a) 256*1024 hilos en total
b) 32 hilos en un warp
c) 256
d) Global Loads: 2    Global stores: 1
e) 256 + (128 + 64 + 32 + 16 + 8 + 4 + 2 + 1) * 3 + 256 = 1277
f) Línea 27.
g) En las últimas cinco iteraciones.
h) En la últma línea, hacer que solo un hilo almacene el resultado. Se ahorran 255 accesos por bloque.
6.12 BLOCK_SIZE = 1
6.13
a) Será más eficiente por el corrimiento de bits.
b) Una lectura porque está asumiendo que el tamaño del warp siempre será 32 (falta revisar si se sale de n)
